{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6362e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key_google = os.getenv(\"GEMINI_API_KEY\")\n",
    "api_key_openai = os.getenv(\"OPENAI_API_KEY\")\n",
    "api_key_claude = os.getenv(\"CLAUDE_API_KEY\")\n",
    "api_key_fireworks = os.getenv('FIREWORKS_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a32d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for gemini api call\n",
    "import asyncio\n",
    "import google.generativeai as genai\n",
    "from typing import List\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from tqdm.asyncio import tqdm\n",
    "import nest_asyncio\n",
    "import sys\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "genai.configure(api_key=api_key_google)\n",
    "model = genai.GenerativeModel('gemini-2.5-flash')  \n",
    "\n",
    "async def call_gemini(prompt: str, pbar) -> str:\n",
    "    \"\"\"Async Gemini call with error handling\"\"\"\n",
    "    try:\n",
    "        pbar.update(1)\n",
    "        response = await model.generate_content_async(prompt)  \n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        pbar.update(1)\n",
    "        return f\"Error: {str(e)}\"\n",
    "        \n",
    "\n",
    "async def parallel_gemini(prompts: List[str], max_concurrency: int = 10) -> List[str]:\n",
    "    with tqdm(total=len(prompts), desc=\"Calling Gemini\") as pbar:\n",
    "        semaphore = asyncio.Semaphore(max_concurrency)\n",
    "        \n",
    "        async def limited_call(prompt):\n",
    "            async with semaphore:\n",
    "                return await call_gemini(prompt, pbar)\n",
    "        \n",
    "        tasks = [limited_call(prompt) for prompt in prompts]\n",
    "        return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b99a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for gpt api call\n",
    "import asyncio\n",
    "from typing import List\n",
    "from tqdm.asyncio import tqdm\n",
    "from openai import AsyncOpenAI\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "model_name=\"o4-mini\"\n",
    "client = AsyncOpenAI(api_key=api_key_openai)\n",
    "\n",
    "async def call_openai(prompt: str, pbar) -> str:\n",
    "    \"\"\"Async OpenAI call with error handling\"\"\"\n",
    "    try:\n",
    "        pbar.update(1)\n",
    "        response = await client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        pbar.update(1)\n",
    "        return f\"Error: {str(e)}\"\n",
    "    \n",
    "async def parallel_openai(prompts: List[str], max_concurrency: int = 5) -> List[str]:\n",
    "    with tqdm(total=len(prompts), desc=\"Calling OpenAI GPT-4o-mini\") as pbar:\n",
    "        semaphore = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "        async def limited_call(prompt):\n",
    "            async with semaphore:\n",
    "                return await call_openai(prompt, pbar)\n",
    "\n",
    "        tasks = [limited_call(prompt) for prompt in prompts]\n",
    "        return await asyncio.gather(*tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b730a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for claude api call\n",
    "import asyncio\n",
    "from typing import List\n",
    "from tqdm.asyncio import tqdm\n",
    "from anthropic import AsyncAnthropic, HUMAN_PROMPT, AI_PROMPT\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Claude Client Setup\n",
    "client = AsyncAnthropic(api_key=api_key_claude)\n",
    "\n",
    "model_name = \"claude-sonnet-4-20250514\"\n",
    "\n",
    "\n",
    "async def call_claude(prompt: str, pbar) -> str:\n",
    "    \"\"\"Async call to Claude with streaming enabled\"\"\"\n",
    "    try:\n",
    "        pbar.update(1)\n",
    "        response = await client.messages.create(\n",
    "            model=model_name,\n",
    "            max_tokens=8192,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "\n",
    "        return response.content[0].text.strip()\n",
    "    except Exception as e:\n",
    "        pbar.update(1)\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "async def parallel_claude(prompts: List[str], max_concurrency: int = 5) -> List[str]:\n",
    "    with tqdm(total=len(prompts), desc=\"Calling Claude\") as pbar:\n",
    "        semaphore = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "        async def limited_call(prompt):\n",
    "            async with semaphore:\n",
    "                return await call_claude(prompt, pbar)\n",
    "\n",
    "        tasks = [limited_call(prompt) for prompt in prompts]\n",
    "        return await asyncio.gather(*tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for deepseek api call\n",
    "import asyncio\n",
    "from typing import List\n",
    "from tqdm.asyncio import tqdm\n",
    "from openai import AsyncOpenAI\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "model_name=\"accounts/fireworks/models/deepseek-r1-basic\"\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"https://api.fireworks.ai/inference/v1\",\n",
    "    api_key= api_key_fireworks,\n",
    ")\n",
    "\n",
    "async def call_deepseek(prompt: str, pbar) -> str:\n",
    "    \"\"\"Async DeepSeek call with error handling\"\"\"\n",
    "    try:\n",
    "        pbar.update(1)\n",
    "        stream = await client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=5000000000000,\n",
    "            stream=True  \n",
    "        )\n",
    "\n",
    "        full_response = \"\"\n",
    "        async for chunk in stream:\n",
    "            if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:\n",
    "                full_response += chunk.choices[0].delta.content\n",
    "\n",
    "        return full_response.strip()\n",
    "    except Exception as e:\n",
    "        pbar.update(1)\n",
    "        return f\"Error: {str(e)}\"\n",
    "    \n",
    "async def parallel_deepseek(prompts: List[str], max_concurrency: int = 5) -> List[str]:\n",
    "    with tqdm(total=len(prompts), desc=\"Calling DeepSeek R1\") as pbar:\n",
    "        semaphore = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "        async def limited_call(prompt):\n",
    "            async with semaphore:\n",
    "                return await call_deepseek(prompt, pbar)\n",
    "\n",
    "        tasks = [limited_call(prompt) for prompt in prompts]\n",
    "        return await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a12d3cd",
   "metadata": {},
   "source": [
    "## Regular Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f528f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../ExecBench/Livecodebench_regular_inputs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    inputs = json.load(f)\n",
    "\n",
    "expected_outputs = list()\n",
    "prompts = list()\n",
    "q_ids = list()\n",
    "for i in tqdm(range(len(inputs))):\n",
    "        prompt = f\"\"\"{inputs[i]['code']}\n",
    "\n",
    "run the code above and infer the output from the code and provided input, including a detailed, step-by-step explanation of the thinking process and each step of the data flow and control flow that led to your conclusion. print the final output in the last line in the following format: output=answer\"\"\"\n",
    "        prompts.append(prompt)\n",
    "        expected_outputs.append(inputs[i]['execution_output'])\n",
    "        q_ids.append(inputs[i]['q_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13705b6b",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad52f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "async def main():\n",
    "    responses = await parallel_gemini(prompts, max_concurrency=5)\n",
    "    return responses \n",
    "\n",
    "responses = asyncio.run(main())\n",
    "\n",
    "records = list()\n",
    "for i in range(len(responses)):\n",
    "   \n",
    "    record = {\n",
    "        \"question_id\": str(q_ids[i]),\n",
    "        \"prompt\": prompts[i],\n",
    "        \"llm_output\": responses[i], \n",
    "        \"expected_output\": expected_outputs[i],                \n",
    "        }\n",
    "    records.append(record)\n",
    "\n",
    "with open(\"llm_reasoning_gemini_livecodebench_regular.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(records, file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ec7894",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    responses = await parallel_openai(prompts, max_concurrency=5)\n",
    "    return responses \n",
    "\n",
    "responses = asyncio.run(main())\n",
    "\n",
    "records = list()\n",
    "for i in range(len(responses)):\n",
    "   \n",
    "    record = {\n",
    "        \"question_id\": str(q_ids[i]),\n",
    "        \"prompt\": prompts[i],\n",
    "        \"llm_output\": responses[i], \n",
    "        \"expected_output\": expected_outputs[i],                \n",
    "        }\n",
    "    records.append(record)\n",
    "\n",
    "with open(\"llm_reasoning_gpt_livecodebench_regular.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(records, file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97931cf4",
   "metadata": {},
   "source": [
    "### Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e16d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    responses = await parallel_claude(prompts, max_concurrency=5)\n",
    "    return responses\n",
    "\n",
    "responses = asyncio.run(main())\n",
    "\n",
    "records = list()\n",
    "for i in range(len(responses)):\n",
    "   \n",
    "    record = {\n",
    "        \"question_id\": str(q_ids[i]),\n",
    "        \"prompt\": prompts[i],\n",
    "        \"llm_output\": responses[i], \n",
    "        \"expected_output\": expected_outputs[i],                \n",
    "        }\n",
    "    records.append(record)\n",
    "\n",
    "with open(\"llm_reasoning_claude_livecodebench_regular.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(records, file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab723b",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c36f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    responses = await parallel_deepseek(prompts, max_concurrency=5)\n",
    "    return responses \n",
    "\n",
    "responses = asyncio.run(main())\n",
    "\n",
    "records = list()\n",
    "for i in range(len(responses)):\n",
    "   \n",
    "    record = {\n",
    "        \"question_id\": str(q_ids[i]),\n",
    "        \"prompt\": prompts[i],\n",
    "        \"llm_output\": responses[i], \n",
    "        \"expected_output\": expected_outputs[i],                \n",
    "        }\n",
    "    records.append(record)\n",
    "\n",
    "with open(\"llm_reasoning_deepseek_livecodebench_regular.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(records, file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54901057",
   "metadata": {},
   "source": [
    "## Edge Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d85787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../ExecBench/Livecodebench_edge_inputs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    inputs = json.load(f)\n",
    "\n",
    "expected_outputs = list()\n",
    "prompts = list()\n",
    "q_ids = list()\n",
    "for i in tqdm(range(len(inputs))):\n",
    "        prompt = f\"\"\"{inputs[i]['code']}\n",
    "\n",
    "run the code above and infer the output from the code and provided input, including a detailed, step-by-step explanation of the thinking process and each step of the data flow and control flow that led to your conclusion. print the final output in the last line in the following format: output=answer\"\"\"\n",
    "        prompts.append(prompt)\n",
    "        expected_outputs.append(inputs[i]['execution_output'])\n",
    "        q_ids.append(inputs[i]['q_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded9874",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec1b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "async def main():\n",
    "    responses = await parallel_gemini(prompts, max_concurrency=5)\n",
    "    return responses \n",
    "\n",
    "responses = asyncio.run(main())\n",
    "\n",
    "records = list()\n",
    "for i in range(len(responses)):\n",
    "   \n",
    "    record = {\n",
    "        \"question_id\": str(q_ids[i]),\n",
    "        \"prompt\": prompts[i],\n",
    "        \"llm_output\": responses[i], \n",
    "        \"expected_output\": expected_outputs[i],                \n",
    "        }\n",
    "    records.append(record)\n",
    "\n",
    "with open(\"llm_reasoning_gemini_livecodebench_edge.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(records, file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7d9b95",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f58299",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    responses = await parallel_openai(prompts, max_concurrency=5)\n",
    "    return responses \n",
    "\n",
    "responses = asyncio.run(main())\n",
    "\n",
    "records = list()\n",
    "for i in range(len(responses)):\n",
    "   \n",
    "    record = {\n",
    "        \"question_id\": str(q_ids[i]),\n",
    "        \"prompt\": prompts[i],\n",
    "        \"llm_output\": responses[i], \n",
    "        \"expected_output\": expected_outputs[i],                \n",
    "        }\n",
    "    records.append(record)\n",
    "\n",
    "with open(\"llm_reasoning_gpt_livecodebench_edge.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(records, file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf1e74b",
   "metadata": {},
   "source": [
    "### Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0315612",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    responses = await parallel_claude(prompts, max_concurrency=5)\n",
    "    return responses\n",
    "\n",
    "responses = asyncio.run(main())\n",
    "\n",
    "records = list()\n",
    "for i in range(len(responses)):\n",
    "   \n",
    "    record = {\n",
    "        \"question_id\": str(q_ids[i]),\n",
    "        \"prompt\": prompts[i],\n",
    "        \"llm_output\": responses[i], \n",
    "        \"expected_output\": expected_outputs[i],                \n",
    "        }\n",
    "    records.append(record)\n",
    "\n",
    "with open(\"llm_reasoning_claude_livecodebench_edge.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(records, file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415b44ab",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    responses = await parallel_deepseek(prompts, max_concurrency=5)\n",
    "    return responses \n",
    "\n",
    "responses = asyncio.run(main())\n",
    "\n",
    "records = list()\n",
    "for i in range(len(responses)):\n",
    "   \n",
    "    record = {\n",
    "        \"question_id\": str(q_ids[i]),\n",
    "        \"prompt\": prompts[i],\n",
    "        \"llm_output\": responses[i], \n",
    "        \"expected_output\": expected_outputs[i],                \n",
    "        }\n",
    "    records.append(record)\n",
    "\n",
    "with open(\"llm_reasoning_deepseek_livecodebench_edge.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(records, file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da3ff1",
   "metadata": {},
   "source": [
    "## Invalid Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../ExecBench/Livecodebench_invalid_inputs.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    inputs = json.load(f)\n",
    "\n",
    "prompts = list()\n",
    "q_ids = list()\n",
    "for i in tqdm(range(len(inputs))):\n",
    "        prompt = f\"\"\"{inputs[i]['code']}\n",
    "\n",
    "run the code above and infer the output from the code and provided input, including a detailed, step-by-step explanation of the thinking process and each step of the data flow and control flow that led to your conclusion. print the final output in the last line in the following format: output=answer\"\"\"\n",
    "        prompts.append(prompt)\n",
    "        q_ids.append(inputs[i]['q_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99dc091",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd701ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "async def main():\n",
    "    responses = await parallel_gemini(prompts, max_concurrency=5)\n",
    "    return responses \n",
    "\n",
    "responses = asyncio.run(main())\n",
    "\n",
    "records = list()\n",
    "for i in range(len(responses)):\n",
    "   \n",
    "    record = {\n",
    "        \"question_id\": str(q_ids[i]),\n",
    "        \"prompt\": prompts[i],\n",
    "        \"llm_output\": responses[i], \n",
    "        \"expected_output\": \"Error\",                \n",
    "        }\n",
    "    records.append(record)\n",
    "\n",
    "with open(\"llm_reasoning_gemini_livecodebench_invalid.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(records, file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaa4f30",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3220d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    responses = await parallel_openai(prompts, max_concurrency=5)\n",
    "    return responses \n",
    "\n",
    "responses = asyncio.run(main())\n",
    "\n",
    "records = list()\n",
    "for i in range(len(responses)):\n",
    "   \n",
    "    record = {\n",
    "        \"question_id\": str(q_ids[i]),\n",
    "        \"prompt\": prompts[i],\n",
    "        \"llm_output\": responses[i], \n",
    "        \"expected_output\": \"Error\",}\n",
    "    records.append(record)\n",
    "\n",
    "with open(\"llm_reasoning_gpt_livecodebench_invalid.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(records, file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b98d3",
   "metadata": {},
   "source": [
    "### Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    responses = await parallel_claude(prompts, max_concurrency=5)\n",
    "    return responses\n",
    "\n",
    "responses = asyncio.run(main())\n",
    "\n",
    "records = list()\n",
    "for i in range(len(responses)):\n",
    "   \n",
    "    record = {\n",
    "        \"question_id\": str(q_ids[i]),\n",
    "        \"prompt\": prompts[i],\n",
    "        \"llm_output\": responses[i], \n",
    "        \"expected_output\": \"Error\"\n",
    "        }\n",
    "    records.append(record)\n",
    "\n",
    "with open(\"llm_reasoning_claude_livecodebench_invalid.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(records, file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454f0c63",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e16163",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    responses = await parallel_deepseek(prompts, max_concurrency=5)\n",
    "    return responses \n",
    "\n",
    "responses = asyncio.run(main())\n",
    "\n",
    "records = list()\n",
    "for i in range(len(responses)):\n",
    "   \n",
    "    record = {\n",
    "        \"question_id\": str(q_ids[i]),\n",
    "        \"prompt\": prompts[i],\n",
    "        \"llm_output\": responses[i], \n",
    "        \"expected_output\": \"Error\"\n",
    "        }\n",
    "    records.append(record)\n",
    "\n",
    "with open(\"llm_reasoning_deepseek_livecodebench_invalid.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(records, file, indent=4, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
