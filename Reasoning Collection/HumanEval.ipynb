{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52682bb",
   "metadata": {},
   "source": [
    "## Regular Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af7f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef4eba34",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df93104",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba2bdd",
   "metadata": {},
   "source": [
    "### Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d5a577",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee650b99",
   "metadata": {},
   "source": [
    "## Edge Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f82adbf",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0fc74",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd8ec5",
   "metadata": {},
   "source": [
    "### Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09177860",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c29e6",
   "metadata": {},
   "source": [
    "## Invalid Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99f1b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('evalplus/humanevalplus')['test']\n",
    "\n",
    "from tqdm import tqdm \n",
    "from itertools import pairwise\n",
    "from builtins import pow\n",
    "from heapq import heappush\n",
    "import re\n",
    "\n",
    "expected_outputs = list()\n",
    "prompts = list()\n",
    "input_strs = list()\n",
    "inputss = list()\n",
    "q_ids = list()\n",
    "with open('livecodebench/InvalidTestCasesHumanEval.txt', 'r') as f:    \n",
    "    tests = f.read()\n",
    "tests = tests.split('*************************************************************\\n')\n",
    "\n",
    "for ind in range(len(dataset)):\n",
    "    t = tests[ind].split('\\n')\n",
    "    for j in [0, 2]:\n",
    "        input_str = dataset[ind]['prompt'] + dataset[ind]['canonical_solution'] + '\\n'+ t[j]\n",
    "        prompt = f\"\"\"{input_str}\n",
    "\n",
    "        run the code above and infer the output from the code and provided input, including a detailed, step-by-step explanation of the thinking process and each step of the data flow and control flow that led to your conclusion. print the final output in the last line in the following format: output=answer\"\"\"\n",
    "        prompts.append(prompt)\n",
    "        q_ids.append(dataset[ind]['task_id'])\n",
    "        #inputss.append(tss[6:-1])\n",
    "        input_strs.append(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96430ed5",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b8581",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c126e53",
   "metadata": {},
   "source": [
    "### Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed50144",
   "metadata": {},
   "source": [
    "### DeepSeek"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
